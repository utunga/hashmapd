
#list of sections, below, that should retain values from here when using the override
#(otherwise you have to explictly define all keys you need in the override places)
sections_to_merge: ['raw', 'couchdb','train', 'shape', 'tsne','output'],

environment:
{
    hashmapd: '../hashmapd'
}
raw:
{
    words_with_coords: 'data/word_coords_utf8.csv' 
    couch_server_url: 'http://127.0.0.1:5984'
    couch_db: 'hashmapd'
    request_queue_db: 'hashmapd'
    raw_data_file: 'data/fake_txt/raw_data.json'
    max_simultaneous_requests:5
}
couchdb:
{
    server_url: 'http://127.0.0.1:5984'
    database: 'hashmapd'
}
input:
{
    csv_data: 'data/user_word_vectors.csv'
    csv_contains_pixels: False
    csv_contains_counts: True
    number_of_examples:5285
    number_for_training:4400
    number_for_validation:600
    number_for_testing: $input.number_of_examples-$input.number_for_training-$input.number_for_validation
    input_words: 'data/words.csv'
    train_data_info: 'data/word_vectors_info.pkl.gz'
    train_data: 'data/word_vectors_' # data split into train, validate, and test across several files
    render_data: 'data/all_word_vectors.pkl.gz' #data in one numpy array, for purposes of rendering
    render_data_has_labels: False
},
shape:
{
    input_vector_length: 3000
    mid_layer_sizes: [512,128]
    inner_code_length: 32
},
train:
{
	method:'cd'
	k:1
	first_layer_type:'bernoulli'
	noise_std_dev:0.0
	cost:'squared_diff'
    weights_file:'data/word_vectors_weights.pkl.gz'
    n_ins: $shape.input_vector_length
    skip_trace_images: True
    train_batch_size:30 
    pretraining_epochs:1000
    training_epochs:1000
},
tsne:
{
    desired_dims: 2 
    pca_dims : $shape.input_vector_length #set this to inner_code_length to effectively skip the PCA step (i hope)
    perplexity: 30 #roughly 'the optimal number of neighbours'
    max_iter: 200 # NOT CURRENTLY SUPPORTED (default is 1000)
},
output:
{
    coords_file: 'out/coords.csv'
    labels_file: 'out/labels.csv'
    codes_file: 'out/codes.csv'
}

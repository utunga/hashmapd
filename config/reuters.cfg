
sections_to_merge: ['raw', 'couchdb','train', 'shape', 'tsne','output'],

input:
{
    csv_data: 'data/fake_txt_word_vectors.csv'
    words_file: 'out/fake_words.csv'
    users_file: 'out/fake_users.csv'
    number_of_examples:6000#5000
    number_for_training:5000#3700
    number_for_validation:500#800
    number_for_testing: $input.number_of_examples-$input.number_for_training-$input.number_for_validation
    train_data_info: 'data/reuters/reuters_5000_data_info.pkl.gz'
    train_data: 'data/reuters/reuters_5000_' # data split into train, validate, and test across several files
    render_data: $input.train_data_info
    render_data_has_labels: True
    
},
shape:
{
    input_vector_length: 2000 # num words to find 
    mid_layer_sizes: [500,500]
    inner_code_length: 128
}
,
train:
{
	method:'cd'
	k:1
	first_layer_type:'bernoulli'
	noise_std_dev:0.0
	cost:'squared_diff'
    weights_file: "data/reuters_weights.pkl.gz"
    skip_trace_during_training:False
    skip_trace_images:True
    train_batch_size:100
    pretraining_epochs:30#150
    training_epochs:50#500
},
output:
{
    coords_file: 'out/reuters_coords.csv'
    labels_file: 'out/reuters_labels.csv'
    codes_file: 'out/reuters_codes.csv'
}
